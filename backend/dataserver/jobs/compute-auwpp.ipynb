{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright © 2019 Sunho Kim. All rights reserved.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gorani/gorani/backend/dataserver\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Compute UWPP')\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "uw_threshold = 2\n",
    "words_in_page = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                name|             content|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|                  It|[table, title, pa...|\n",
      "|  2|If It’s for My Da...|[table, cover, pr...|\n",
      "|  4|If It’s for My Da...|[table, cover, yo...|\n",
      "|  5|To Kill A Mocking...|[dedication, part...|\n",
      "|  6|            Twilight|[first, sight, mo...|\n",
      "|  3|If It’s for My Da...|[table, cover, yo...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gorani.spark import read_data_all\n",
    "\n",
    "df = read_data_all(spark, 'books', cache=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|             content|original_len|\n",
      "+---+--------------------+------------+\n",
      "|  1|[table, title, pa...|         250|\n",
      "|  1|[peter, buick, ri...|         250|\n",
      "|  1|[final, showdown,...|         250|\n",
      "|  1|[along, diagonal,...|         250|\n",
      "|  1|[jerk, darkness, ...|         250|\n",
      "|  1|[would, hear, som...|         250|\n",
      "|  1|[life, thick, phl...|         250|\n",
      "|  1|[hair, eye, slit,...|         250|\n",
      "|  1|[shit, dismayed, ...|         250|\n",
      "|  1|[indeed, therefor...|         250|\n",
      "|  1|[although, forty-...|         250|\n",
      "|  1|[time, found, imp...|         250|\n",
      "|  1|[wa, without, muc...|         250|\n",
      "|  1|[one, touch, wa, ...|         250|\n",
      "|  1|[wa, said, tissue...|         250|\n",
      "|  1|[wa, would, surpr...|         250|\n",
      "|  1|[amazing, quiet, ...|         250|\n",
      "|  1|[engineering, fir...|         250|\n",
      "|  1|[said, tell, know...|         250|\n",
      "|  1|[im, prize, even,...|         250|\n",
      "+---+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def _slice_content(col, words_in_page):\n",
    "    return list(chunks(col, words_in_page))\n",
    "\n",
    "slice_content = F.udf(_slice_content, T.ArrayType(T.ArrayType(T.StringType())))\n",
    "\n",
    "page_df = df.select('id', F.explode(slice_content('content', F.lit(words_in_page))).alias('content'))\n",
    "page_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from gorani.spark import write_data\n",
    "df2 = read_data_all(spark, 'user_known_words')\n",
    "user_known_maps = df2.rdd.map(lambda x: (x['user_id'], (x['word'], x['score'])))\\\n",
    "        .groupByKey().map(lambda x: (x[0], dict((y, z) for y, z in x[1])))\\\n",
    "        .collect()\n",
    "user_known_maps_bd = sc.broadcast(user_known_maps)\n",
    "uw_threshold_bd = sc.broadcast(uw_threshold)\n",
    "words_in_page_bd = sc.broadcast(words_in_page)\n",
    "\n",
    "def known(known_map, word):\n",
    "    if word not in known_map:\n",
    "        return 0\n",
    "    return known_map[word]\n",
    "\n",
    "def uwpp(row):\n",
    "    return [Row(user_id=id,\\\n",
    "                uwpp=len([word for word in row['content'] if known(nmap, word) <= uw_threshold_bd.value])/words_in_page_bd.value,\\\n",
    "                book_id=row['id']) for id, nmap in user_known_maps_bd.value]\n",
    "uwpp_df = page_df.rdd.flatMap(uwpp).toDF()\n",
    "res_df = uwpp_df.groupBy('book_id', 'user_id').agg(F.mean('uwpp').alias('auwpp'))\n",
    "\n",
    "write_data('user_auwpps', res_df)\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
