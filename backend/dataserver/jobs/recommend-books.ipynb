{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright Â© 2019 Sunho Kim. All rights reserved.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gorani/gorani/backend/dataserver\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Recommend Books')\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "book_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+----------------+----+--------------------+--------------------+\n",
      "|                  id|user_id|target_id|            kind|rate|          created_at|          updated_at|\n",
      "+--------------------+-------+---------+----------------+----+--------------------+--------------------+\n",
      "|ae142f49-b63e-443...|     10|        2|recommended_book|  -1|2019-05-25 21:13:...|2019-05-25 22:42:...|\n",
      "+--------------------+-------+---------+----------------+----+--------------------+--------------------+\n",
      "\n",
      "+-------+-------+\n",
      "|user_id|book_id|\n",
      "+-------+-------+\n",
      "+-------+-------+\n",
      "\n",
      "+-------+-------+\n",
      "|user_id|book_id|\n",
      "+-------+-------+\n",
      "+-------+-------+\n",
      "\n",
      "+---+-------+-------+----------+----------+\n",
      "| id|user_id|book_id|created_at|updated_at|\n",
      "+---+-------+-------+----------+----------+\n",
      "+---+-------+-------+----------+----------+\n",
      "\n",
      "+---+-------+\n",
      "| id|cluster|\n",
      "+---+-------+\n",
      "|  5|      1|\n",
      "|  6|      1|\n",
      "|  1|      0|\n",
      "|  2|      0|\n",
      "|  4|      1|\n",
      "|  3|      1|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gorani.spark import read_api_all, read_data_all\n",
    "\n",
    "rate_df = read_api_all(spark, 'rates')\\\n",
    "        .where('kind = \"recommended_book\"')\n",
    "rate_df.show()\n",
    "\n",
    "eb_df = read_data_all(spark, 'experienced_books')\n",
    "eb_df.show()\n",
    "\n",
    "rb_df = read_data_all(spark, 'readable_books')\n",
    "rb_df.show()\n",
    "\n",
    "rcb_df = read_api_all(spark, 'recommended_books')\n",
    "rcb_df.show()\n",
    "\n",
    "cluster_df = read_data_all(spark, 'book_cluster')\n",
    "cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+---+-------+\n",
      "|user_id|cluster|count|sum|old_rec|\n",
      "+-------+-------+-----+---+-------+\n",
      "|     10|      0|    0|  0|      0|\n",
      "+-------+-------+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "cluster_count_df = rate_df.join(cluster_df, rate_df['target_id'] == cluster_df['id'], 'inner')\\\n",
    "    .select(F.col('target_id').alias('book_id'), 'user_id', 'cluster', 'rate')\\\n",
    "    .groupBy('user_id', 'cluster').agg(F.sum('rate').alias('count'))\\\n",
    "    .select('user_id', 'cluster', F.when(F.col('count') < 0, F.lit(0)).otherwise(F.col('count').cast(IntegerType())).alias('count'))\n",
    "\n",
    "cluster_count_sum_df = cluster_count_df.groupBy('user_id')\\\n",
    "                    .agg(F.sum('count').alias('sum'))\n",
    "\n",
    "cluster_count_df = cluster_count_df.join(cluster_count_sum_df.alias('f'),\\\n",
    "                    cluster_count_df['user_id'] == cluster_count_sum_df['user_id'], 'left')\\\n",
    "                    .drop(F.col('f.user_id'))\n",
    "\n",
    "rcbn_df = rcb_df.groupBy('user_id')\\\n",
    "    .agg(F.count(F.lit(1)).alias('old_rec'))\\\n",
    "\n",
    "cluster_count_df = cluster_count_df.join(rcbn_df.alias('f2'),\\\n",
    "        cluster_count_df['user_id'] == rcbn_df['user_id'], 'left')\\\n",
    "        .drop(F.col('f2.user_id'))\\\n",
    "        .select('user_id', 'cluster', 'count', 'sum', F.when(F.isnull('old_rec'), 0).otherwise(F.col('old_rec')).alias('old_rec'))\n",
    "\n",
    "cluster_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|cluster|user_id|book_id|\n",
      "+-------+-------+-------+\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = cluster_df.join(rb_df, cluster_df['id'] == rb_df['book_id'], 'inner').drop(F.col('id'))\n",
    "df = df.join(eb_df, (df['book_id'] == eb_df['book_id']) & (df['user_id'] == eb_df['user_id']), 'left_anti')\n",
    "candidate_df = df.join(rate_df, (df['book_id'] == rate_df['target_id']) & (df['user_id'] == rate_df['user_id']), 'left_anti')\n",
    "candidate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+----+\n",
      "|user_id|cluster|old_rec|need|\n",
      "+-------+-------+-------+----+\n",
      "|     10|      0|      0|   0|\n",
      "+-------+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "need_cluster_df = cluster_count_df.select('user_id', 'cluster', 'old_rec',\\\n",
    "(F.col('count') / F.when(F.col('sum') == 0, F.lit(0.1)).otherwise(F.col('sum'))\\\n",
    "* (F.lit(book_number) - F.col('old_rec'))).cast(IntegerType()).alias('need'))\n",
    "\n",
    "need_cluster_df.show()\n",
    "need_cluster = need_cluster_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "+---+\n",
      "\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "from gorani.spark import write_api\n",
    "from gorani.utils import uuid\n",
    "\n",
    "out = []\n",
    "for row in need_cluster:\n",
    "    out += candidate_df.where((F.col('cluster') == row['cluster']) & (F.col('user_id') == row['user_id']))\\\n",
    "                .orderBy(F.rand())\\\n",
    "                .limit(row['need'])\\\n",
    "                .drop(F.col('cluster'))\\\n",
    "                .collect()\n",
    "    if row['need'] != (book_number - row['old_rec']):\n",
    "        need = book_number - row['need']\n",
    "        out += candidate_df\\\n",
    "            .orderBy(F.rand())\\\n",
    "            .limit(need)\\\n",
    "            .drop(F.col('cluster'))\\\n",
    "            .collect()\n",
    "\n",
    "if len(out) != 0:\n",
    "    result_df = spark.createDataFrame(out)\n",
    "    result_df = result_df.withColumn('id', uuid())\\\n",
    "        .withColumn('updated_at', F.current_timestamp())\\\n",
    "        .withColumn('created_at', F.current_timestamp())\n",
    "    write_api('recommended_books', result_df)\n",
    "    result_df.show()\n",
    "\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
