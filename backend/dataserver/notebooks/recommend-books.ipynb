{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('gorani.zip')\n",
    "sc.addPyFile('gorani.zip')\n",
    "from gorani import firebase\n",
    "firebase.init('spark')\n",
    "mydb = firebase.db()\n",
    "from gorani.gorani import Gorani\n",
    "from gorani.transformer import Transformer\n",
    "from gorani.utils import split_sentence\n",
    "gorani = Gorani(mydb)\n",
    "transformer = Transformer(gorani, spark, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from gorani.transformer import piper\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"clean_logs.json\")\n",
    "df = df.filter(df['scorePerc'] >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indDf = df.n(transformer.parse_time())\\\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\\\n",
    "    .withColumn('sentence', F.explode('sentences'))\\\n",
    "    .withColumn('sid', F.col('sentence.sid'))\\\n",
    "    .withColumn('unknown', F.when(F.col('sentence.unknown') == True, 1).otherwise(0))\\\n",
    "    .withColumn('wordCount', F.size('sentence.words'))\\\n",
    "    .withColumn('uwordCount', F.size('sentence.unknownWords'))\n",
    "\n",
    "senDf = indDf\\\n",
    "    .groupBy('id', 'userId', 'eltime', 'sentences', 'scorePerc', 'timeZ', 'classId', 'chapterId', 'bookId').agg(F.sum('unknown').alias('usenCount'), \n",
    "                       F.sum('wordCount').alias('wordCount'), \n",
    "                       F.sum('uwordCount').alias('uwordCount'))\\\n",
    "    .drop('id').withColumn('wpm', F.col('wordCount')/(F.col('eltime')/(1000*60))).filter(F.col('wpm') < 1000)\\\n",
    "    .filter(F.col('eltime') < 5*60*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(IntegerType())\n",
    "def uscore(x,y):\n",
    "    import pandas as pd\n",
    "    return pd.Series([1 if y in x else 0 for x, y in zip(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "vocabDf = indDf\\\n",
    "    .withColumn('sid', F.col('sentence.sid'))\\\n",
    "    .withColumn('wordCount', F.size('sentence.words'))\\\n",
    "    .withColumn('word', F.explode('sentence.words'))\\\n",
    "    .withColumn('no', uscore('sentence.unknownWords', F.col('word')))\\\n",
    "    .withColumn('yes', 1 - F.col('no'))\\\n",
    "    .withColumn('word', F.lower(F.col('word')))\\\n",
    "    .withColumn('word', transformer.stem('word'))\\\n",
    "    .groupBy('classId', 'userId', 'word')\\\n",
    "    .agg(F.sum('no').alias('ucount'), F.sum('yes').alias('ncount'))\\\n",
    "    .select('classId', 'userId', 'word', 'ucount', 'ncount')\\\n",
    "\n",
    "nwordDf = vocabDf.filter((F.col('ncount')-5*F.col('ucount')) > 0).drop('ncount').drop('ucount')\n",
    "\n",
    "uwordDf = vocabDf.filter((F.col('ncount')-5*F.col('ucount')) <= 0).drop('ncount').drop('ucount')\n",
    "\n",
    "\n",
    "#                 .groupBy('userId')\\\n",
    "#                 .agg(F.collect_list('word').alias('words'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(ArrayType(StringType()))\n",
    "def split_setence_udf(sen):\n",
    "    from gorani.utils import split_sentence\n",
    "    import pandas as pd\n",
    "    return pd.Series([split_sentence(sen) for sen in sen])\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "totalCountDf = transformer.booksDf.withColumn('sentence', transformer.get_sentence('bookId', 'chapterId', 'sid'))\\\n",
    "        .withColumn('sentence', split_setence_udf('sentence'))\\\n",
    "        .withColumn('word', F.explode('sentence'))\\\n",
    "        .drop('sentence')\\\n",
    "        .withColumn('word', F.lower(F.col('word')))\\\n",
    "        .withColumn('word', transformer.stem('word'))\\\n",
    "        .drop('chapterId').drop('sid')\\\n",
    "        .dropDuplicates(['bookId', 'word'])\\\n",
    "        .withColumn('totalCount', F.count('*').over(Window.partitionBy('bookId')))\n",
    "\n",
    "npercDf = totalCountDf.join(nwordDf, ['word'], 'inner')\\\n",
    "        .groupBy('classId', 'userId', 'bookId', 'totalCount')\\\n",
    "        .agg(F.count('*').alias('count'))\\\n",
    "        .withColumn('nperc', F.col('count')/F.col('totalCount'))\\\n",
    "        .drop('totalCount').drop('count')\n",
    "\n",
    "upercDf = totalCountDf.join(uwordDf, ['word'], 'inner')\\\n",
    "        .groupBy('classId', 'userId', 'bookId', 'totalCount')\\\n",
    "        .agg(F.count('*').alias('count'))\\\n",
    "        .withColumn('uperc', F.col('count')/F.col('totalCount'))\\\n",
    "        .drop('totalCount').drop('count')\n",
    "\n",
    "percDf = npercDf.join(upercDf, ['classId', 'userId', 'bookId'], 'full')\\\n",
    "                .withColumn('uperc', F.when(F.col('uperc').isNull(), 0).otherwise(F.col('uperc')))\\\n",
    "                .withColumn('nperc', F.when(F.col('nperc').isNull(), 0).otherwise(F.col('nperc')))\\\n",
    "                .withColumn('eperc', 1 - F.col('uperc') - F.col('nperc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "window = Window.partitionBy('classId', 'bookId').orderBy(F.col('uperc').desc())\n",
    "\n",
    "classPercDf = percDf.groupBy('classId', 'bookId')\\\n",
    "    .agg(F.avg('eperc').alias('eperc'), F.avg('nperc').alias('nperc'), F.avg('uperc').alias('uperc'))\n",
    "\n",
    "classStruggleDf = percDf\\\n",
    "    .select('classId', 'bookId', 'userId', F.row_number().over(window).alias('rank'))\\\n",
    "    .filter(F.col('rank') <= 2)\\\n",
    "    .withColumn('username', transformer.get_username(F.col('userId')))\\\n",
    "    .groupBy('classId', 'bookId')\\\n",
    "    .agg(F.collect_list('username').alias('struggles'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classDf = classPercDf.join(classStruggleDf, ['classId', 'bookId'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = classDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set([row['classId'] for row in rows]))\n",
    "out = {\n",
    "    classId: [\n",
    "        {\n",
    "            'bookId': row['bookId'],\n",
    "            'eperc': row['eperc'],\n",
    "            'nperc': row['nperc'],\n",
    "            'uperc': row['uperc'],\n",
    "            'struggles': row['struggles']\n",
    "        }\n",
    "        for row in rows if row['classId'] == classId\n",
    "    ]\n",
    "    for classId in classes\n",
    "}\n",
    "\n",
    "for classId, res in out.items():\n",
    "    mydb.collection('dataResult').document(classId).set({'recommendedBooks': res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"words\", minSupport=0.8, minConfidence=0.6)\n",
    "model = fpGrowth.fit(uwordDf)\n",
    "model.associationRules.orderBy(F.col('lift').desc()).show(1000, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Spark 2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
