{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('gorani.zip')\n",
    "sc.addPyFile('gorani.zip')\n",
    "from gorani import firebase\n",
    "firebase.init('spark')\n",
    "mydb = firebase.db()\n",
    "from gorani.gorani import Gorani\n",
    "from gorani.transformer import Transformer\n",
    "from gorani.utils import split_sentence\n",
    "gorani = Gorani(mydb)\n",
    "transformer = Transformer(gorani, spark, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from gorani.transformer import piper\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "c = -50\n",
    "@F.udf(IntegerType())\n",
    "def readingScore(ac, wpm):\n",
    "    if ac > 0.9 and wpm > 250 + c:\n",
    "        return 12\n",
    "    elif ac > 0.85 and wpm > 237 + c:\n",
    "        return 11\n",
    "    elif ac > 0.825 and wpm > 224 + c:\n",
    "        return 10\n",
    "    elif ac > 0.8 and wpm > 214 + c: \n",
    "        return 9\n",
    "    elif ac > 0.7 and wpm > 204 + c:\n",
    "        return 8\n",
    "    elif ac > 0.6 and wpm > 195 + c:\n",
    "        return 7\n",
    "    elif ac > 0.55 and wpm > 185 + c:\n",
    "        return 6\n",
    "    elif ac > 0.5 and wpm > 173 + c:\n",
    "        return 5\n",
    "    elif ac > 0.475 and wpm > 158 + c:\n",
    "        return 4\n",
    "    elif ac > 0.45 and wpm > 138 + c:\n",
    "        return 3\n",
    "    elif ac > 0.3 and wpm > 115 + c:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "@F.udf(IntegerType())\n",
    "def vocabScore(uv):\n",
    "    v = (1 - uv)*100\n",
    "    if v >= 99:\n",
    "        return 12\n",
    "    elif v >= 96:\n",
    "        return 11\n",
    "    elif v > 93:\n",
    "        return 10\n",
    "    elif v > 90: \n",
    "        return 9\n",
    "    elif v > 87:\n",
    "        return 8\n",
    "    elif v > 84:\n",
    "        return 7\n",
    "    elif v > 81:\n",
    "        return 6\n",
    "    elif v > 78:\n",
    "        return 5\n",
    "    elif v > 75:\n",
    "        return 4\n",
    "    elif v > 72:\n",
    "        return 3\n",
    "    elif v > 69:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"clean_logs.json\")\n",
    "df = df.filter(df['scorePerc'] >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(StringType())\n",
    "def ymw(ts):\n",
    "    from datetime import datetime\n",
    "    dt = datetime.fromtimestamp(ts)\n",
    "    return dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "indDf = df.n(transformer.parse_time())\\\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\\\n",
    "    .withColumn(\"ymw\", ymw(\"ts\"))\\\n",
    "    .withColumn('sentence', F.explode('sentences'))\\\n",
    "    .withColumn('sid', F.col('sentence.sid'))\\\n",
    "    .withColumn('unknown', F.when(F.col('sentence.unknown') == True, 1).otherwise(0))\\\n",
    "    .withColumn('wordCount', F.size('sentence.words'))\\\n",
    "    .withColumn('uwordCount', F.size('sentence.unknownWords'))\n",
    "\n",
    "senDf = indDf\\\n",
    "    .groupBy('id', 'ymw', 'userId', 'eltime', 'sentences', 'scorePerc', 'timeZ', 'classId', 'chapterId', 'bookId').agg(F.sum('unknown').alias('usenCount'), \n",
    "                       F.sum('wordCount').alias('wordCount'), \n",
    "                       F.sum('uwordCount').alias('uwordCount'))\\\n",
    "    .drop('id').withColumn('wpm', F.col('wordCount')/(F.col('eltime')/(1000*60))).filter(F.col('wpm') < 1000)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(IntegerType())\n",
    "def uscore(x,y):\n",
    "    import pandas as pd\n",
    "    return pd.Series([1 if y in x else 0 for x, y in zip(x,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('userId').orderBy(F.col('ymw')).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "userYmwDf = senDf\\\n",
    "    .groupBy('ymw','classId','userId')\\\n",
    "    .agg(F.count('*').alias('count'), F.sum('wpm').alias('wpm'), F.sum('scorePerc').alias('scorePerc'), F.sum('eltime').alias('time'))\\\n",
    "    .select('ymw', 'classId', 'userId', 'time',\n",
    "            (F.sum('wpm').over(window) / F.sum('count').over(window)).alias('wpm'),\n",
    "            (F.sum('scorePerc').over(window) / F.sum('count').over(window)).alias('scorePerc'))\\\n",
    "    .withColumn('rc', readingScore('scorePerc','wpm'))\\\n",
    "    .withColumn('username', transformer.get_username('userId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBookDf = senDf.filter(F.col('eltime') < 5*60*1000)\\\n",
    "    .groupBy('classId','bookId','userId')\\\n",
    "    .agg(F.count('*').alias('count'), F.avg('wpm').alias('wpm'), F.avg('scorePerc').alias('scorePerc'), F.sum('uwordCount').alias('uwordCount'), F.sum('eltime').alias('time'), F.avg('timeZ').alias('timeZ'))\\\n",
    "    .withColumn('rc', readingScore('scorePerc','wpm'))\\\n",
    "    .withColumn('username', transformer.get_username('userId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDf = senDf.filter(F.col('eltime') < 5*60*1000)\\\n",
    "    .groupBy('classId','userId')\\\n",
    "    .agg(F.count('*').alias('count'), F.avg('wpm').alias('wpm'), F.avg('scorePerc').alias('scorePerc'), F.sum('uwordCount').alias('uwordCount'), F.sum('eltime').alias('time'), F.avg('timeZ').alias('timeZ'))\\\n",
    "    .withColumn('rc', readingScore('scorePerc','wpm'))\\\n",
    "    .withColumn('username', transformer.get_username('userId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@piper\n",
    "def calculate_uperc(df, args):\n",
    "    return df.withColumn('sentence', F.explode('sentences'))\\\n",
    "    .withColumn('sid', F.col('sentence.sid'))\\\n",
    "    .withColumn('wordCount', F.size('sentence.words'))\\\n",
    "    .withColumn('word', F.explode('sentence.words'))\\\n",
    "    .withColumn('no', uscore('sentence.unknownWords', F.col('word')))\\\n",
    "    .withColumn('yes', 1 - F.col('no'))\\\n",
    "    .withColumn('word', F.lower(F.col('word')))\\\n",
    "    .dropDuplicates(args + ['userId', 'word', 'yes', 'no'])\\\n",
    "    .groupBy(*(['userId', 'word']+args))\\\n",
    "    .agg(F.sum('yes').alias('yes'), F.sum('no').alias('no'))\\\n",
    "    .withColumn('yes', F.when((F.col('yes') == 1) & (F.col('no') == 0), 1).otherwise(0))\\\n",
    "    .groupBy(*(['userId']+args))\\\n",
    "    .agg(F.sum('yes').alias('nwordCount'), F.sum('no').alias('uwordCount'))\\\n",
    "    .withColumn('uperc', F.col('uwordCount') / (F.col('uwordCount') + F.col('nwordCount')))\\\n",
    "    .select(*(['userId', 'uperc'] + args))\\\n",
    "    .withColumn('vc', vocabScore('uperc'))\n",
    "\n",
    "window = Window.partitionBy('userId').orderBy(F.col('ymw')).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "@piper\n",
    "def calculate_uperc_ymw(df):\n",
    "    return df.withColumn('sentence', F.explode('sentences'))\\\n",
    "    .withColumn('sid', F.col('sentence.sid'))\\\n",
    "    .withColumn('wordCount', F.size('sentence.words'))\\\n",
    "    .withColumn('word', F.explode('sentence.words'))\\\n",
    "    .withColumn('no', uscore('sentence.unknownWords', F.col('word')))\\\n",
    "    .withColumn('yes', 1 - F.col('no'))\\\n",
    "    .withColumn('word', F.lower(F.col('word')))\\\n",
    "    .dropDuplicates(['userId', 'word', 'yes', 'no'])\\\n",
    "    .groupBy('userId', 'word', 'ymw')\\\n",
    "    .agg(F.sum('yes').alias('yes'), F.sum('no').alias('no'))\\\n",
    "    .withColumn('yes', F.when((F.col('yes') == 1) & (F.col('no') == 0), 1).otherwise(0))\\\n",
    "    .groupBy('userId', 'ymw')\\\n",
    "    .agg(F.sum('yes').alias('nwordCount'), F.sum('no').alias('uwordCount'))\\\n",
    "    .withColumn('uperc', F.sum('uwordCount').over(window) / (F.sum('uwordCount').over(window) + F.sum('nwordCount').over(window)))\\\n",
    "    .select('userId', 'uperc', 'ymw')\\\n",
    "    .withColumn('vc', vocabScore('uperc'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymwDf = senDf.n(calculate_uperc_ymw())\\\n",
    "    .join(userYmwDf, ['userId', 'ymw'], 'inner')\\\n",
    "    .orderBy('userId', 'ymw')\n",
    "\n",
    "bookDf = senDf.n(calculate_uperc(['bookId']))\\\n",
    "    .join(userBookDf, ['userId', 'bookId'], 'inner')\\\n",
    "    .select('userId', 'bookId', 'wpm', 'rc', 'vc', 'uperc', 'scorePerc', 'count')\n",
    "\n",
    "allDf = senDf.n(calculate_uperc([]))\\\n",
    "    .join(userDf, ['userId'], 'inner')\\\n",
    "    .withColumn('bookId', F.lit('all'))\\\n",
    "    .select('userId', 'bookId', 'wpm', 'rc', 'vc', 'uperc', 'scorePerc', 'count')\n",
    "\n",
    "allBookDf = bookDf.union(allDf)\n",
    "\n",
    "userIdToAge = {user.id: user.to_dict().get('age', -1) for user in mydb.collection('users').stream()}\n",
    "\n",
    "@F.udf(IntegerType())\n",
    "def age(user):\n",
    "    return int(userIdToAge.get(user, 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "window = Window.partitionBy(F.col('userId')).orderBy(F.col('unknown').desc(), F.col('uwordCount').desc())\n",
    "\n",
    "usenDf = indDf\\\n",
    "    .select('userId', 'bookId', 'chapterId', 'sid', 'unknown', 'uwordCount', F.row_number().over(window).alias('rank'))\\\n",
    "    .filter(~(F.col('unknown') == 0) | ~(F.col('uwordCount') == 0))\\\n",
    "    .filter(F.col('rank') <= 10)\\\n",
    "    .withColumn('sentence', transformer.get_sentence('bookId', 'chapterId', 'sid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "window = Window.partitionBy(F.col('userId')).orderBy(F.col('count').desc())\n",
    "\n",
    "uwordDf = indDf\\\n",
    "    .withColumn('sid', F.col('sentence.sid'))\\\n",
    "    .withColumn('wordCount', F.size('sentence.words'))\\\n",
    "    .withColumn('word', F.explode('sentence.words'))\\\n",
    "    .withColumn('no', uscore('sentence.unknownWords', F.col('word')))\\\n",
    "    .withColumn('yes', 1 - F.col('no'))\\\n",
    "    .withColumn('word', F.lower(F.col('word')))\\\n",
    "    .groupBy('userId', 'word')\\\n",
    "    .agg(F.sum('no').alias('count'))\\\n",
    "    .select('userId', 'word', 'count', F.row_number().over(window).alias('rank'))\\\n",
    "    .filter(F.col('rank') <= 10)\\\n",
    "    .filter(~(F.col('count') == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = uwordDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(set([row['userId'] for row in rows]))\n",
    "out = {\n",
    "    user: [\n",
    "    {\n",
    "        'word': row['word'],\n",
    "        'count': row['count']\n",
    "    }  for row in rows if row['userId'] == user]\n",
    "    for user in users\n",
    "}\n",
    "\n",
    "userIdToClass = dict()\n",
    "for userId, res in out.items():\n",
    "    if userId not in userIdToClass:\n",
    "        user = mydb.collection('users').document(userId).get().to_dict()\n",
    "        userIdToClass[userId] = user['classId']\n",
    "    ref = mydb.collection('dataResult').document(\n",
    "        userIdToClass[userId]).collection('serverComputed').document(userId)\n",
    "    try:\n",
    "        doc = ref.get().to_dict()\n",
    "        doc['unknownWords'] = res\n",
    "        ref.set(doc)\n",
    "    except:\n",
    "        ref.set({\n",
    "            'unknownWords': res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = usenDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(set([row['userId'] for row in rows]))\n",
    "out = {\n",
    "    user: [\n",
    "    {\n",
    "        'sentence': row['sentence'],\n",
    "        'uwordCount': row['uwordCount'],\n",
    "        'unknown': row['unknown'],\n",
    "        'bookId': row['bookId']\n",
    "    }  for row in rows if row['userId'] == user]\n",
    "    for user in users\n",
    "}\n",
    "\n",
    "userIdToClass = dict()\n",
    "for userId, res in out.items():\n",
    "    if userId not in userIdToClass:\n",
    "        user = mydb.collection('users').document(userId).get().to_dict()\n",
    "        userIdToClass[userId] = user['classId']\n",
    "    ref = mydb.collection('dataResult').document(\n",
    "        userIdToClass[userId]).collection('serverComputed').document(userId)\n",
    "    try:\n",
    "        doc = ref.get().to_dict()\n",
    "        doc['unknownSentences'] = res\n",
    "        ref.set(doc)\n",
    "    except:\n",
    "        ref.set({\n",
    "            'unknownSentences': res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ymwDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(set([row['userId'] for row in rows]))\n",
    "out = {\n",
    "    user: {\n",
    "        'vc': [{'x': row['ymw'], 'y': row['vc']} for row in rows if row['userId'] == user ],\n",
    "        'rc': [{'x': row['ymw'], 'y': row['rc']} for row in rows if row['userId'] == user ],\n",
    "        'score': [{'x': row['ymw'], 'y': row['scorePerc']} for row in rows if row['userId'] == user ],\n",
    "        'wpm': [{'x': row['ymw'], 'y': row['wpm']} for row in rows if row['userId'] == user ],\n",
    "        'uperc': [{'x': row['ymw'], 'y': row['uperc']} for row in rows if row['userId'] == user ]\n",
    "    }\n",
    "    for user in users\n",
    "}\n",
    "\n",
    "userIdToClass = dict()\n",
    "for userId, res in out.items():\n",
    "    if userId not in userIdToClass:\n",
    "        user = mydb.collection('users').document(userId).get().to_dict()\n",
    "        userIdToClass[userId] = user['classId']\n",
    "    ref = mydb.collection('dataResult').document(\n",
    "        userIdToClass[userId]).collection('serverComputed').document(userId)\n",
    "    try:\n",
    "        doc = ref.get().to_dict()\n",
    "        doc['ymwPerformance'] = res\n",
    "        ref.set(doc)\n",
    "    except:\n",
    "        ref.set({\n",
    "            'ymwPerformance': res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = senDf.groupBy('userId', 'ymw').agg(F.count('*').alias('count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(set([row['userId'] for row in rows]))\n",
    "out = {\n",
    "    user: [{'day': row['ymw'], 'value': row['count']} for row in rows if row['userId'] == user ]\n",
    "    for user in users\n",
    "}\n",
    "\n",
    "userIdToClass = dict()\n",
    "for userId, res in out.items():\n",
    "    if userId not in userIdToClass:\n",
    "        user = mydb.collection('users').document(userId).get().to_dict()\n",
    "        userIdToClass[userId] = user['classId']\n",
    "    ref = mydb.collection('dataResult').document(\n",
    "        userIdToClass[userId]).collection('serverComputed').document(userId)\n",
    "    try:\n",
    "        doc = ref.get().to_dict()\n",
    "        doc['activity'] = res\n",
    "        ref.set(doc)\n",
    "    except:\n",
    "        ref.set({\n",
    "            'activity': res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = allBookDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(set([row['userId'] for row in rows]))\n",
    "out = {\n",
    "    user: {\n",
    "        row['bookId']: {\n",
    "            'vc': row['vc'],\n",
    "            'rc': row['rc'],\n",
    "            'uperc': row['uperc'],\n",
    "            'wpm': row['wpm'],\n",
    "            'score': row['scorePerc'],\n",
    "            'count': row['count']\n",
    "        }\n",
    "        for row in rows if row['userId'] == user\n",
    "    }\n",
    "    for user in users\n",
    "}\n",
    "\n",
    "userIdToClass = dict()\n",
    "for userId, res in out.items():\n",
    "    if userId not in userIdToClass:\n",
    "        user = mydb.collection('users').document(userId).get().to_dict()\n",
    "        userIdToClass[userId] = user['classId']\n",
    "    ref = mydb.collection('dataResult').document(\n",
    "        userIdToClass[userId]).collection('serverComputed').document(userId)\n",
    "    try:\n",
    "        doc = ref.get().to_dict()\n",
    "        doc['bookPerformance'] = res\n",
    "        ref.set(doc)\n",
    "    except:\n",
    "        ref.set({\n",
    "            'bookPerformance': res\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Spark 2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
