{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "\n",
    "# The problem Gorani Reader wants to solve\n",
    "\n",
    "For each user, we want to calculate the probability that the user knows some word i,\n",
    "\n",
    "$ \\hat y =  P(Know) $\n",
    "\n",
    "## Knowledge tracing\n",
    "\n",
    "Knowledge can't be observed directly by sensor. So, in typical education data mining researches, this is inferred by observing students' attempts to solve relevant problems. We call this approach \"Knowledge Tracing.\" One of the most popular knowledge tracing frameworks is Bayesian Knowledge Tracing, which models $ \\hat y $ using forward probability from hidden markov model.\n",
    "\n",
    "$ \\hat y_t(k) = P(o_1,o_2, ..., o_t, Know_t=k) = \\sum^1_i=0 \\hat y_{t-1}(i) p(Know_t=k|Know_{t-1}=i) p(o_t|Know_{t-1}=i) $ \n",
    "\n",
    "Where $o_i = 1$ if the answer to problem was right 0 otherwise.\n",
    "\n",
    "The modell will learn $ p(Know_t=k|Know_{t-1}=i) $, $ p(o_t|Know_{t-1}=i) $, and initial state probability for each \"skill\" (e.g. quadratic formula, square root in algebra subject) \n",
    "\n",
    "\n",
    "In Gorani Reader, I applied this model using $o_i = 0$ if the user looked up some word j otherwise 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two distinct charateristics of vocabulary tracing that might resulted in poor performance of Bayesian Knowledge Tracing for Gorani Reader.\n",
    "\n",
    "1. There are too many \"skills\" The number of skills is the same as the number of words or stems.\n",
    "\n",
    "BKT usually suffer when the number of skills is high. \n",
    "\n",
    "2. Few attempts at each skill. User might be not able to encounter some unusal word many times. \n",
    "\n",
    "Lack of observations to .\n",
    "\n",
    "\n",
    "# My hypothesis\n",
    "\n",
    "In Amazon Kindle, the student can look up the word which he or she want to know the definition of by holding onto that word. When I was studying english using Amazon Kindle (9th grades), one thought came to my mind: I am labelling unknown words by using ditionary look up function. We can express this hypothesis with following equation.\n",
    "\n",
    "$ 1 - \\hat y = 1 - P(Know) = P(Unknown) = P(Cliked) $\n",
    "\n",
    "This is quite a strong assumption. But, with this strong assumption, we can utilize a standard machine learning approach to model user's knowledge. We simply predict whether user clicks word i or not (ground truth)\n",
    "\n",
    "$ \\hat y = f(x;\\theta) $ where x is a vector constructed from various features extracted from user's pagination log. \n",
    "\n",
    "$ y = 0 $ if user looked up word i $1$ otherwise.  \n",
    "\n",
    "## Experiment\n",
    "\n",
    "I conducted an experiment to verify this experiment. For the sake of detailed analysis, I splitted down the P(Known) and P(Unknown).\n",
    "\n",
    "$ P(Known) = P(Known|Clicked)\\cdot P(Clicked) + P(Known|NotClicked)\\cdot P(NotClicked)$ \n",
    "\n",
    "$ P(Unknown) = P(Unknown|Clicked)\\cdot P(Clicked) + P(Unknown|NotClicked)\\cdot P(NotClicked)$ \n",
    "\n",
    "Notice that when conditional probabilities are \n",
    "\n",
    "$ P(Known|Clicked) = 0 $\n",
    "\n",
    "$ P(Known|NotClicked) = 1 $\n",
    "\n",
    "$ P(Unknown|Clicked) = 1 $\n",
    "\n",
    "$ P(Unknown|NotClicked) = 0 $\n",
    "\n",
    "The objective probabilities become\n",
    "\n",
    "$ P(Known) = P(NotClicked) $\n",
    "\n",
    "$ P(Unknown) = P(Clicked) $\n",
    "\n",
    "Which is my hypothesis (user clicks every unknown word)\n",
    "\n",
    "I estimated these conditional probabilities and looked at how close are estimated conditional probabilites to our hypothetical values. (0,1,1,0)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ML model\n",
    "\n",
    "Event logs from Gorani Reader is rather simple. \n",
    "\n",
    "\n",
    "We're going to transform this logs using external information.\n",
    "\n",
    "\n",
    "I collected the logs from total 76 students to traind and validate these ML models.\n",
    "\n",
    "# First version\n",
    "\n",
    "\n",
    "\n",
    "## Eye tracking \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second version\n",
    "\n",
    "\n",
    "## Bayesian tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third version\n",
    "\n",
    "\n",
    "## SunhoDiffVec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final verison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
