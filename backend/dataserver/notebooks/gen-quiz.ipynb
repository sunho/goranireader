{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sunhokim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunhokim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = pd.read_csv('concrete.csv', delimiter = ',')\n",
    "con_df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(sen):\n",
    "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "  return sen_new\n",
    "def summarize_sentences(sentences, n):\n",
    "  clean_sentences = [{'id': sen['id'], 'content': sen['content'].replace(\"[^a-zA-Z]\", \" \")} for sen in sentences]\n",
    "  clean_sentences = [{'id': s['id'], 'content': s['content'].lower()} for s in clean_sentences]\n",
    "  clean_sentences = [{'id': s['id'], 'content': remove_stopwords(s['content'].split())} for s in clean_sentences]\n",
    "  sentence_vectors = []\n",
    "  for d in clean_sentences:\n",
    "    i = d['content']\n",
    "    if len(i) != 0:\n",
    "      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "      v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "\n",
    "  sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "  from sklearn.metrics.pairwise import cosine_similarity\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "      if i != j:\n",
    "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "  import networkx as nx\n",
    "\n",
    "  nx_graph = nx.from_numpy_array(sim_mat)\n",
    "  scores = nx.pagerank(nx_graph)\n",
    "  ranked_sentences = sorted(((scores[i],s['id'], s['content']) for i,s in enumerate(sentences)), reverse=True)\n",
    "  return [{'id': ranked_sentences[i][1], 'content': ranked_sentences[i][2]} for i in range(min(n,len(ranked_sentences)))]\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gorani.models.quiz import Quiz, WordItem, Chapter\n",
    "import simplejson\n",
    "import re\n",
    "book = None\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "with open('book1.book', 'rb') as f:\n",
    "  book = simplejson.load(f)\n",
    "fdist = FreqDist()\n",
    "for chap in book['chapters']:\n",
    "  for item in chap['items']:\n",
    "    words = word_tokenize(item['content'])\n",
    "    words = pd.Series(words).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    words = [remove_stopwords(r.split()) for r in words]\n",
    "    for word in words:\n",
    "      word2 = lemmatizer.lemmatize(word.lower())\n",
    "      fdist[word2] += 1\n",
    "    \n",
    "def get_surrounding_text(chapid, senid):\n",
    "    chap = next(filter(lambda x: x['id'] == chapid, book['chapters']), None)\n",
    "    if chap == None:\n",
    "        raise Exception(\"wtf\")\n",
    "    for i in range(0, len(chap['items'])):\n",
    "        if chap['items'][i]['id'] == senid:\n",
    "           return ' '.join(s['content'] for s in chap['items'][max(0,i-2):i]) + ' ' + chap['items'][i]['content']\n",
    "    raise Exception(\"wtf\")\n",
    "\n",
    "chap_sens = [{'id': chap['id'], 'sens': summarize_sentences(chap['items'], 10)} for chap in book['chapters']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3ed2c894-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ed2cd80-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ed570ee-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ed7eb26-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ed9d8aa-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3edc55f8-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ede17b2-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ee1062a-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ee3c90a-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ee565b2-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ee7c41a-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3eeb301e-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3eef685a-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ef4504a-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3ef8f60e-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3efbab74-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3efc2fa4-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3efe64cc-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f009cec-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f02772e-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f03f89c-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f06fe0c-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f084d0c-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f0900c6-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f0a0110-dc06-11e9-8569-48bf6beb4102 completed\n",
      "3f0b4c28-dc06-11e9-8569-48bf6beb4102 completed\n"
     ]
    }
   ],
   "source": [
    "for chap in chap_sens:\n",
    "    for sen in chap['sens']:\n",
    "        keyword = None\n",
    "        ki = 0\n",
    "        \n",
    "        minScore = 9999999999999999999999\n",
    "        words = re.split(\"[^a-zA-Z-]+\",sen['content'])\n",
    "        i = -1\n",
    "        for word in words:\n",
    "            if word == '':\n",
    "                continue\n",
    "            i +=1\n",
    "            word2 = lemmatizer.lemmatize(word.lower())\n",
    "            if word2 in stop_words:\n",
    "                continue\n",
    "            try:\n",
    "                con = con_df.loc[word2]['conc']\n",
    "            except KeyError:\n",
    "                con = 5\n",
    "            if con is not None:\n",
    "                if len(wordnet.synsets(word2)) >= 3:\n",
    "                    if  con < minScore:\n",
    "                        minScore = con\n",
    "                        keyword = word\n",
    "                        ki = i\n",
    "            \n",
    "        if keyword != None:\n",
    "            syns = wordnet.synsets(keyword)\n",
    "            anws = [s.definition() for s in syns]\n",
    "            items.append(WordItem(str(uuid.uuid1()),get_surrounding_text(chap[\"id\"], sen[\"id\"]),ki,anws,0,keyword))\n",
    "    out.chapters.append(Chapter(chap['id'], items))\n",
    "    print(chap['id'] + ' completed')\n",
    "from xml.dom import minidom\n",
    "buf = minidom.parseString(out.toXML().decode('utf-8')).toprettyxml(indent=\"   \")\n",
    "with open('book2.quiz', 'wb') as f:\n",
    "  f.write(buf.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make reference to\n",
      "be relevant to\n",
      "think of, regard, or classify under a subsuming principle or with a general group or in relation to another\n",
      "send or direct for treatment, information, or a decision\n",
      "seek information from\n",
      "have as a meaning\n",
      "use a name to designate\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets(\"refer\"):\n",
    "    print(syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    )\n",
    "\n",
    "\n",
    "out = Quiz()\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
