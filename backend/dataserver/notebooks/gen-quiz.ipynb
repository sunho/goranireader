{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sunhokim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunhokim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = pd.read_csv('concrete.csv', delimiter = ',')\n",
    "con_df.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(sen):\n",
    "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "  return sen_new\n",
    "def summarize_sentences(sentences, n):\n",
    "  if len(sentences) <= 2:\n",
    "    return []\n",
    "  clean_sentences = [{'id': sen['id'], 'content': sen['content'].replace(\"[^a-zA-Z]\", \" \")} for sen in sentences]\n",
    "  clean_sentences = [{'id': s['id'], 'content': s['content'].lower()} for s in clean_sentences]\n",
    "  clean_sentences = [{'id': s['id'], 'content': remove_stopwords(s['content'].split())} for s in clean_sentences]\n",
    "  sentence_vectors = []\n",
    "  for d in clean_sentences:\n",
    "    i = d['content']\n",
    "    if len(i) != 0:\n",
    "      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "      v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "\n",
    "  sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "  from sklearn.metrics.pairwise import cosine_similarity\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "      if i != j:\n",
    "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "  import networkx as nx\n",
    "  try:\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "  except:\n",
    "    return []\n",
    "  ranked_sentences = sorted(((scores[i],s['id'], s['content']) for i,s in enumerate(sentences)), reverse=True)\n",
    "  return [{'id': ranked_sentences[i][1], 'content': ranked_sentences[i][2]} for i in range(min(n,len(ranked_sentences)))]\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gorani.models.quiz import Quiz, WordItem, Chapter\n",
    "import simplejson\n",
    "import re\n",
    "book = None\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "with open('book2.book', 'rb') as f:\n",
    "  book = simplejson.load(f)\n",
    "fdist = FreqDist()\n",
    "for chap in book['chapters']:\n",
    "  for item in chap['items']:\n",
    "    words = word_tokenize(item['content'])\n",
    "    words = pd.Series(words).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    words = [remove_stopwords(r.split()) for r in words]\n",
    "    for word in words:\n",
    "      word2 = lemmatizer.lemmatize(word.lower())\n",
    "      fdist[word2] += 1\n",
    "    \n",
    "def get_surrounding_text(chapid, senid):\n",
    "    chap = next(filter(lambda x: x['id'] == chapid, book['chapters']), None)\n",
    "    if chap == None:\n",
    "        raise Exception(\"wtf\")\n",
    "    for i in range(0, len(chap['items'])):\n",
    "        if chap['items'][i]['id'] == senid:\n",
    "           return ' '.join(s['content'] for s in chap['items'][max(0,i-2):i])\n",
    "    raise Exception(\"wtf\")\n",
    "\n",
    "# chap_sens = [{'id': chap['id'], 'sens': summarize_sentences(chap['items'], 10)} for chap in book['chapters']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gorani.models.quiz.Quiz object at 0x12af62b50>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_definitions(word):\n",
    "    return [s.definition() for s in wordnet.synsets(word)]\n",
    "\n",
    "def cal_score(word):\n",
    "    try:\n",
    "        con = con_df.loc[word]['conc']\n",
    "    except KeyError:\n",
    "        con = 5\n",
    "    return con * 5 + sum([1 if w.pos() == 'n' else 0 for w in wordnet.synsets(word)]) + fdist.freq(word) * 100000\n",
    "    \n",
    "def split_sentence(sen):\n",
    "    out = re.split(\"[^a-zA-Z-]+\",sen)\n",
    "    \n",
    "    if out[0] == '':\n",
    "        if len(out) == 1:\n",
    "            return []\n",
    "        out = out[1:]\n",
    "    if out[len(out) - 1] == '':\n",
    "        if len(out) == 1:\n",
    "            return []\n",
    "        out = out[:-1]\n",
    "    return out\n",
    "\n",
    "out = Quiz()\n",
    "\n",
    "for chap in book['chapters']:\n",
    "    items = list()\n",
    "    for sen in chap['items']:\n",
    "        keyword = None\n",
    "        ki = 0\n",
    "        minScore = 9999999999999999999999\n",
    "        words = split_sentence(sen['content'])\n",
    "        i = -1\n",
    "        for word in words:\n",
    "            i += 1\n",
    "            word2 = lemmatizer.lemmatize(word.lower())\n",
    "            if word2 in stop_words:\n",
    "                continue\n",
    "            con = cal_score(word2)\n",
    "            if con is not None:\n",
    "                if len(get_definitions(word)) >= 3 and len(get_definitions(word)) <= 10:\n",
    "                    if con < minScore:\n",
    "                        minScore = con\n",
    "                        keyword = word\n",
    "                        ki = i \n",
    "        if keyword != None:\n",
    "            anws = get_definitions(keyword)\n",
    "            up = get_surrounding_text(chap[\"id\"], sen[\"id\"])\n",
    "            ki += len(split_sentence(up))\n",
    "            items.append((minScore, WordItem(str(uuid.uuid1()), up + ' ' + sen[\"content\"], ki, anws, 0, keyword)))\n",
    "#             print(keyword, minScore)\n",
    "    items = [item[1] for item in sorted(items, key=lambda x: x[0])]\n",
    "    if len(items) > 10:\n",
    "        items = items[:10]\n",
    "    \n",
    "    out.chapters.append(Chapter(chap['id'], items))\n",
    "#     print(chap['id'] + ' completed')\n",
    "\n",
    "print(out)\n",
    "from xml.dom import minidom\n",
    "buf = minidom.parseString(out.toXML().decode('utf-8')).toprettyxml(indent=\"   \")\n",
    "with open('book2.quiz', 'wb') as f:\n",
    "  f.write(buf.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make reference to\n",
      "be relevant to\n",
      "think of, regard, or classify under a subsuming principle or with a general group or in relation to another\n",
      "send or direct for treatment, information, or a decision\n",
      "seek information from\n",
      "have as a meaning\n",
      "use a name to designate\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets(\"refer\"):\n",
    "    print(syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    )\n",
    "\n",
    "\n",
    "out = Quiz()\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
