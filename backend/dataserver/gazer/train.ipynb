{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1121c4450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n/dennis_jade.pickle\n",
      "n/sunho_early1.pickle\n",
      "c/dennis_jade.pickle\n",
      "c/sunho_early1.pickle\n",
      "c/sunho_early2.pickle\n",
      "ic/dennis_jade.pickle\n",
      "ic/sunho_early1.pickle\n",
      "ic/sunho_early2.pickle\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "def loadall(filenames):\n",
    "    out = []\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            out.extend(pickle.load(f))\n",
    "    return out\n",
    "\n",
    "ndata = loadall(glob('n/*'))\n",
    "cdata = loadall(glob('c/*'))\n",
    "icdata = loadall(glob('ic/*'))\n",
    "\n",
    "x = ndata + cdata + icdata\n",
    "# n 0 c 1 ic 2\n",
    "y = [0] * len(ndata) + [1] * len(cdata) + [2] * len(icdata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_size, n_layers,\\\n",
    "                 drop_lstm=0.1, drop_out = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(5, hidden_dim, n_layers, \n",
    "                            dropout=drop_lstm, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "                \n",
    "        # pack, remove pads\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, seq_lengths.cpu().numpy(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
    "          # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html\n",
    "          # If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero\n",
    "\n",
    "        # unpack, recover padded sequence\n",
    "        output, input_sizes = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "       \n",
    "        # collect the last output in each batch\n",
    "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
    "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output).squeeze()\n",
    "               \n",
    "        # sigmoid function\n",
    "        output = self.sig(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3367, 0.3789, 0.2844],\n",
      "        [0.3512, 0.3652, 0.2835],\n",
      "        [0.3588, 0.3637, 0.2776],\n",
      "        [0.3442, 0.3765, 0.2793],\n",
      "        [0.3749, 0.3596, 0.2655],\n",
      "        [0.3470, 0.3716, 0.2813],\n",
      "        [0.3466, 0.3735, 0.2799],\n",
      "        [0.3597, 0.3669, 0.2734],\n",
      "        [0.3435, 0.3783, 0.2782],\n",
      "        [0.3480, 0.3709, 0.2811],\n",
      "        [0.3478, 0.3725, 0.2796],\n",
      "        [0.3456, 0.3754, 0.2789],\n",
      "        [0.3433, 0.3781, 0.2785],\n",
      "        [0.3510, 0.3609, 0.2881],\n",
      "        [0.3596, 0.3665, 0.2739],\n",
      "        [0.3574, 0.3711, 0.2715],\n",
      "        [0.3598, 0.3666, 0.2736],\n",
      "        [0.3544, 0.3762, 0.2693],\n",
      "        [0.3410, 0.3835, 0.2755],\n",
      "        [0.3452, 0.3755, 0.2793],\n",
      "        [0.3509, 0.3661, 0.2830],\n",
      "        [0.3497, 0.3671, 0.2832],\n",
      "        [0.3575, 0.3688, 0.2737],\n",
      "        [0.3608, 0.3685, 0.2707],\n",
      "        [0.3486, 0.3716, 0.2797],\n",
      "        [0.3597, 0.3661, 0.2742],\n",
      "        [0.3630, 0.3556, 0.2813],\n",
      "        [0.3509, 0.3616, 0.2875],\n",
      "        [0.3514, 0.3612, 0.2874],\n",
      "        [0.3436, 0.3774, 0.2791],\n",
      "        [0.3553, 0.3711, 0.2736],\n",
      "        [0.3469, 0.3742, 0.2790],\n",
      "        [0.3571, 0.3734, 0.2695],\n",
      "        [0.3472, 0.3743, 0.2785],\n",
      "        [0.3481, 0.3720, 0.2799],\n",
      "        [0.3510, 0.3606, 0.2883],\n",
      "        [0.3454, 0.3812, 0.2734],\n",
      "        [0.3513, 0.3593, 0.2894],\n",
      "        [0.3361, 0.3785, 0.2853],\n",
      "        [0.3521, 0.3591, 0.2888],\n",
      "        [0.3437, 0.3763, 0.2801],\n",
      "        [0.3505, 0.3698, 0.2796],\n",
      "        [0.3392, 0.3708, 0.2900],\n",
      "        [0.3472, 0.3769, 0.2760],\n",
      "        [0.3669, 0.3509, 0.2822],\n",
      "        [0.3508, 0.3676, 0.2815],\n",
      "        [0.3558, 0.3721, 0.2721],\n",
      "        [0.3565, 0.3702, 0.2732],\n",
      "        [0.3376, 0.3774, 0.2850],\n",
      "        [0.3413, 0.3726, 0.2861],\n",
      "        [0.3703, 0.3571, 0.2726],\n",
      "        [0.3561, 0.3678, 0.2761],\n",
      "        [0.3471, 0.3729, 0.2799],\n",
      "        [0.3516, 0.3598, 0.2886],\n",
      "        [0.3347, 0.3806, 0.2847],\n",
      "        [0.3409, 0.3861, 0.2730],\n",
      "        [0.3506, 0.3694, 0.2800],\n",
      "        [0.3457, 0.3726, 0.2817],\n",
      "        [0.3322, 0.3855, 0.2823],\n",
      "        [0.3503, 0.3692, 0.2806],\n",
      "        [0.3358, 0.3809, 0.2833],\n",
      "        [0.3492, 0.3712, 0.2796],\n",
      "        [0.3328, 0.3864, 0.2808],\n",
      "        [0.3468, 0.3737, 0.2795],\n",
      "        [0.3437, 0.3804, 0.2759],\n",
      "        [0.3482, 0.3713, 0.2805],\n",
      "        [0.3610, 0.3628, 0.2762],\n",
      "        [0.3538, 0.3804, 0.2658],\n",
      "        [0.3478, 0.3721, 0.2801],\n",
      "        [0.3587, 0.3672, 0.2741],\n",
      "        [0.3356, 0.3803, 0.2842],\n",
      "        [0.3347, 0.3831, 0.2822],\n",
      "        [0.3472, 0.3731, 0.2797],\n",
      "        [0.3455, 0.3729, 0.2817],\n",
      "        [0.3435, 0.3785, 0.2780],\n",
      "        [0.3699, 0.3643, 0.2658],\n",
      "        [0.3487, 0.3763, 0.2751],\n",
      "        [0.3488, 0.3725, 0.2787],\n",
      "        [0.3433, 0.3823, 0.2744],\n",
      "        [0.3574, 0.3670, 0.2756],\n",
      "        [0.3451, 0.3733, 0.2816],\n",
      "        [0.3606, 0.3672, 0.2723],\n",
      "        [0.3380, 0.3784, 0.2836]])\n",
      "loss: tensor(1.0846, grad_fn=<NllLossBackward>) epoch: 0\n",
      "loss: tensor(1.0793, grad_fn=<NllLossBackward>) epoch: 1\n",
      "loss: tensor(1.0761, grad_fn=<NllLossBackward>) epoch: 2\n",
      "loss: tensor(1.0734, grad_fn=<NllLossBackward>) epoch: 3\n",
      "loss: tensor(1.0694, grad_fn=<NllLossBackward>) epoch: 4\n",
      "loss: tensor(1.0660, grad_fn=<NllLossBackward>) epoch: 5\n",
      "loss: tensor(1.0617, grad_fn=<NllLossBackward>) epoch: 6\n",
      "loss: tensor(1.0591, grad_fn=<NllLossBackward>) epoch: 7\n",
      "loss: tensor(1.0563, grad_fn=<NllLossBackward>) epoch: 8\n",
      "loss: tensor(1.0529, grad_fn=<NllLossBackward>) epoch: 9\n",
      "loss: tensor(1.0523, grad_fn=<NllLossBackward>) epoch: 10\n",
      "loss: tensor(1.0464, grad_fn=<NllLossBackward>) epoch: 11\n",
      "loss: tensor(1.0440, grad_fn=<NllLossBackward>) epoch: 12\n",
      "loss: tensor(1.0408, grad_fn=<NllLossBackward>) epoch: 13\n",
      "loss: tensor(1.0394, grad_fn=<NllLossBackward>) epoch: 14\n",
      "loss: tensor(1.0360, grad_fn=<NllLossBackward>) epoch: 15\n",
      "loss: tensor(1.0338, grad_fn=<NllLossBackward>) epoch: 16\n",
      "loss: tensor(1.0322, grad_fn=<NllLossBackward>) epoch: 17\n",
      "loss: tensor(1.0298, grad_fn=<NllLossBackward>) epoch: 18\n",
      "loss: tensor(1.0275, grad_fn=<NllLossBackward>) epoch: 19\n",
      "tensor([[0.2375, 0.5681, 0.1944],\n",
      "        [0.2435, 0.5467, 0.2098],\n",
      "        [0.2327, 0.5686, 0.1987],\n",
      "        [0.2493, 0.5564, 0.1943],\n",
      "        [0.2328, 0.5747, 0.1925],\n",
      "        [0.2338, 0.5742, 0.1920],\n",
      "        [0.2467, 0.5426, 0.2106],\n",
      "        [0.2343, 0.5717, 0.1940],\n",
      "        [0.2394, 0.5675, 0.1931],\n",
      "        [0.2503, 0.5392, 0.2104],\n",
      "        [0.2455, 0.5668, 0.1877],\n",
      "        [0.2800, 0.5192, 0.2008],\n",
      "        [0.2339, 0.5722, 0.1938],\n",
      "        [0.2451, 0.5618, 0.1931],\n",
      "        [0.2496, 0.5569, 0.1935],\n",
      "        [0.2309, 0.5772, 0.1919],\n",
      "        [0.2311, 0.5761, 0.1928],\n",
      "        [0.2454, 0.5442, 0.2104],\n",
      "        [0.2326, 0.5746, 0.1928],\n",
      "        [0.2466, 0.5517, 0.2017],\n",
      "        [0.2516, 0.5452, 0.2031],\n",
      "        [0.2505, 0.5429, 0.2066],\n",
      "        [0.2446, 0.5665, 0.1889],\n",
      "        [0.2308, 0.5774, 0.1917],\n",
      "        [0.2486, 0.5586, 0.1928],\n",
      "        [0.2289, 0.5764, 0.1947],\n",
      "        [0.2338, 0.5722, 0.1940],\n",
      "        [0.2472, 0.5583, 0.1945],\n",
      "        [0.2489, 0.5547, 0.1965],\n",
      "        [0.2511, 0.5557, 0.1932],\n",
      "        [0.2285, 0.5738, 0.1977],\n",
      "        [0.2325, 0.5743, 0.1932],\n",
      "        [0.2615, 0.5479, 0.1906],\n",
      "        [0.2331, 0.5725, 0.1944],\n",
      "        [0.2352, 0.5721, 0.1926],\n",
      "        [0.2302, 0.5723, 0.1975],\n",
      "        [0.2538, 0.5469, 0.1993],\n",
      "        [0.2412, 0.5542, 0.2046],\n",
      "        [0.2332, 0.5722, 0.1946],\n",
      "        [0.2438, 0.5606, 0.1955],\n",
      "        [0.2323, 0.5741, 0.1935],\n",
      "        [0.2326, 0.5741, 0.1932],\n",
      "        [0.2531, 0.5526, 0.1944],\n",
      "        [0.2500, 0.5568, 0.1932],\n",
      "        [0.2509, 0.5517, 0.1974],\n",
      "        [0.2374, 0.5643, 0.1983],\n",
      "        [0.2383, 0.5647, 0.1970],\n",
      "        [0.2603, 0.5307, 0.2091],\n",
      "        [0.2511, 0.5504, 0.1985],\n",
      "        [0.2404, 0.5594, 0.2002],\n",
      "        [0.2695, 0.5180, 0.2125],\n",
      "        [0.2407, 0.5602, 0.1991],\n",
      "        [0.2664, 0.5124, 0.2212],\n",
      "        [0.2313, 0.5753, 0.1934],\n",
      "        [0.2466, 0.5525, 0.2008],\n",
      "        [0.2516, 0.5379, 0.2105],\n",
      "        [0.2470, 0.5516, 0.2014],\n",
      "        [0.2479, 0.5558, 0.1964],\n",
      "        [0.2491, 0.5571, 0.1938],\n",
      "        [0.2262, 0.5716, 0.2022],\n",
      "        [0.2529, 0.5520, 0.1951],\n",
      "        [0.2406, 0.5712, 0.1882],\n",
      "        [0.2384, 0.5656, 0.1959],\n",
      "        [0.2336, 0.5717, 0.1947],\n",
      "        [0.2536, 0.5354, 0.2110],\n",
      "        [0.2444, 0.5458, 0.2098],\n",
      "        [0.2777, 0.5098, 0.2125],\n",
      "        [0.2471, 0.5591, 0.1938],\n",
      "        [0.2343, 0.5723, 0.1934],\n",
      "        [0.2484, 0.5511, 0.2005],\n",
      "        [0.2418, 0.5500, 0.2083],\n",
      "        [0.2336, 0.5735, 0.1930],\n",
      "        [0.2341, 0.5729, 0.1931],\n",
      "        [0.2433, 0.5688, 0.1879],\n",
      "        [0.2385, 0.5668, 0.1947],\n",
      "        [0.2602, 0.5220, 0.2178],\n",
      "        [0.2554, 0.5491, 0.1955],\n",
      "        [0.2473, 0.5599, 0.1928],\n",
      "        [0.2549, 0.5509, 0.1942],\n",
      "        [0.2475, 0.5619, 0.1906],\n",
      "        [0.2329, 0.5728, 0.1943],\n",
      "        [0.2327, 0.5743, 0.1930],\n",
      "        [0.2493, 0.5576, 0.1931]])\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = torch.LongTensor(list(map(len, x)))\n",
    "# Add padding(0)\n",
    "seq_tensor = Variable(torch.zeros((len(x), seq_lengths.max(), 5))).float()\n",
    "for idx, (seq, seqlen) in enumerate(zip(x, seq_lengths)):\n",
    "  seq_tensor[idx, :seqlen] = torch.FloatTensor(seq)\n",
    "\n",
    "# targets = torch.nn.functional.one_hot(torch.LongTensor(y))\n",
    "\n",
    "\n",
    "# print(targets)\n",
    "\n",
    "\n",
    "model = Model(10,3,10)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tag_scores = model(seq_tensor, seq_lengths)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(20):  # again, normally you would NOT do 300 epochs, it is toy data:\n",
    "    # Step 1. Remember that Pytorch accumulates gradients.\n",
    "    # We need to clear them out before each instance\n",
    "    model.zero_grad()\n",
    "    \n",
    "    tag_scores = model(seq_tensor, seq_lengths)\n",
    "\n",
    "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "    #  calling optimizer.step()\n",
    "    loss = loss_function(tag_scores, torch.LongTensor(y))\n",
    "    print('loss:', loss, 'epoch:', epoch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    tag_scores = model(seq_tensor, seq_lengths)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
