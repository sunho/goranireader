{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.009322,
     "end_time": "2019-06-03T08:48:17.347085",
     "exception": false,
     "start_time": "2019-06-03T08:48:17.337763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright Â© 2019 Sunho Kim. All rights reserved.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.013618,
     "end_time": "2019-06-03T08:48:17.364334",
     "exception": false,
     "start_time": "2019-06-03T08:48:17.350716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gorani/gorani/backend\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.011975,
     "end_time": "2019-06-03T08:48:17.380181",
     "exception": false,
     "start_time": "2019-06-03T08:48:17.368206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Cluster Books')\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.010053,
     "end_time": "2019-06-03T08:48:17.394034",
     "exception": false,
     "start_time": "2019-06-03T08:48:17.383981",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "k = 2\n",
    "iteration = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.01153,
     "end_time": "2019-06-03T08:48:17.409336",
     "exception": false,
     "start_time": "2019-06-03T08:48:17.397806",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha = 0.6\n",
    "ratio = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 1.452017,
     "end_time": "2019-06-03T08:48:18.865029",
     "exception": false,
     "start_time": "2019-06-03T08:48:17.413012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import CountVectorizer, Normalizer\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "\n",
    "from gorani.spark import read_data_all, write_data\n",
    "from gorani.utils import sparse_to_array\n",
    "\n",
    "SIMILARITY_TYPE = 'cosine'\n",
    "\n",
    "words = read_data_all(spark, 'words', cache = True)\n",
    "books = read_data_all(spark, 'books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 10.198589,
     "end_time": "2019-06-03T08:48:29.067379",
     "exception": false,
     "start_time": "2019-06-03T08:48:18.868790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim mat:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 5, 0.67757248878479), (4, 6, 0.7573773860931396), (1, 2, 0.6806613802909851), (1, 3, 0.6414517760276794), (1, 4, 0.7125809788703918), (1, 5, 0.84701007604599), (1, 6, 0.8728290796279907), (3, 4, 0.9638029932975769), (3, 5, 0.6091843247413635), (3, 6, 0.6786766052246094), (5, 6, 0.8231562972068787), (2, 3, 0.9711453318595886), (2, 4, 0.9739340543746948), (2, 5, 0.6422935128211975), (2, 6, 0.7229636907577515)]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\\\n",
    "    .setInputCol('content')\\\n",
    "    .setOutputCol('tf')\\\n",
    "    .setVocabSize(words.count())\n",
    "cv = cv.fit(books)\n",
    "\n",
    "nor = Normalizer()\\\n",
    "    .setInputCol('tf')\\\n",
    "    .setOutputCol('norm')\n",
    "\n",
    "# normalized book-wordcount matrix\n",
    "tf_mat = nor.transform(cv.transform(books))\n",
    "\n",
    "# convert to SparseMatrix to BlockMatrix\n",
    "mat = IndexedRowMatrix(\n",
    "tf_mat.select(\"id\", \"norm\")\\\n",
    "    .rdd.map(lambda row: IndexedRow(row['id'], row['norm'].toArray())))\\\n",
    "    .toBlockMatrix()\n",
    "\n",
    "# cosine similarity\n",
    "sim_mat_df = mat.multiply(mat.transpose())\\\n",
    "    .toIndexedRowMatrix()\\\n",
    "    .rows.toDF()\n",
    "\n",
    "# change schema\n",
    "sim_af = sim_mat_df.select(F.col('index').alias('id'), F.posexplode(sparse_to_array('vector')))\\\n",
    "    .select('id', F.col('pos').alias('other_id'), F.col('col').alias('value'))\\\n",
    "    .where('id < other_id AND id != 0')\\\n",
    "    .rdd.map(lambda x: tuple([x['id'], x['other_id'], float(x['value'])]))\n",
    "print('sim mat:')\n",
    "print(sim_af.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 2.599071,
     "end_time": "2019-06-03T08:48:31.670560",
     "exception": false,
     "start_time": "2019-06-03T08:48:29.071489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:\n",
      "(id cluster)\n",
      "(4 1)\n",
      "(1 0)\n",
      "(6 1)\n",
      "(3 1)\n",
      "(5 1)\n",
      "(2 0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import PowerIterationClustering, PowerIterationClusteringModel\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model = PowerIterationClustering.train(sim_af, k, iteration)\n",
    "df = model.assignments().toDF()\n",
    "\n",
    "write_data('cluster_books', df)\n",
    "write_data('book_cluster', df)\n",
    "\n",
    "result = model.assignments().collect()\n",
    "print('result:')\n",
    "print('(id cluster)')\n",
    "for item in result:\n",
    "    print('(' + str(item.id) + ' ' + str(item.cluster) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007208,
     "end_time": "2019-06-03T08:48:31.685088",
     "exception": false,
     "start_time": "2019-06-03T08:48:31.677880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "papermill": {
   "duration": 19.05043,
   "end_time": "2019-06-03T08:48:32.873333",
   "environment_variables": {},
   "exception": null,
   "input_path": "jobs/cluster-books.ipynb",
   "output_path": "out/output.ipynb",
   "parameters": {
    "alpha": 0.6,
    "ratio": 0.1
   },
   "start_time": "2019-06-03T08:48:13.822903",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}